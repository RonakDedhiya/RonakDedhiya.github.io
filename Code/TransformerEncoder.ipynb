{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layers\n",
    "\n",
    "Given an input X, a (T, d) dimensional input write a function using numpy to feed it to an encoder and decoder layer of the transformer. You will also have to implement the internal workings of scaled dot-product Attention and Multi-head attention layers. For convenience consider the projection dimension to be the same (dim_size=d) for all Query, Key and Value. Only The forward propagation is expected to be implemented which will be executed by the forward method of each class.\n",
    "<center>\n",
    "<img src=\"./fig/Transformer.png\" width=\"324\" height=\"470\">\n",
    "</center>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./fig/Attention-layers.png\" width=\"550\" height=\"350\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, dim_size):\n",
    "        self.d_k = dim_size\n",
    "        \n",
    "    def forward(self, Q, K, V, Mask=None):\n",
    "        attn_logits = np.einsum(\"lhnk,lhmk->lhnm\",Q,K)/np.sqrt(self.d_k)              # batch x head x seq x seq\n",
    "        if Mask is not None:\n",
    "            attn_logits[Mask==1] = -9e15\n",
    "        \n",
    "        ## Softmax\n",
    "        attention = np.exp(attn_logits)/np.sum(np.exp(attn_logits),axis =-1,keepdims=True)\n",
    "        \n",
    "        ## Matmul -> (batch,head,seq,seq)x(batch,head,seq,head_size) -> (batch,head,seq,head_size)\n",
    "        values = np.einsum(\"lhmn,lhmq->lhmq\",attention,V)                             # batch x head x seq x head_size       \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        # declare the scaled dot product attention layers here\n",
    "        self.heads = num_heads\n",
    "        self.dim_size = dim_size\n",
    "        self.head_size = dim_size//num_heads                                          # head_size = dim_size//n_heads\n",
    "        self.scaled_dot_product =ScaledDotProductAttention(self.head_size)\n",
    "        scale = 1/np.sqrt(self.dim_size)\n",
    "        self.WQ = np.random.uniform(-scale,scale,(self.dim_size,self.dim_size))       # dim_size x dim_size\n",
    "        self.WK = np.random.uniform(-scale,scale,(self.dim_size,self.dim_size))       # dim_size x dim_size\n",
    "        self.WV = np.random.uniform(-scale,scale,(self.dim_size,self.dim_size))       # dim_size x dim_size\n",
    "        self.WO = np.random.uniform(-scale,scale,(self.dim_size,self.dim_size))       # dim_size x dim_size\n",
    "\n",
    "    def forward(self,  Q, K, V, Mask=None):\n",
    "        # returns the forward propagation output\n",
    "        # Q,K,V                                                                       # batch x seq x dim_size\n",
    "        Q = np.dot(Q,self.WQ)                                                         # batch x seq x dim_size\n",
    "        Q = Q.reshape(Q.shape[0],Q.shape[1],self.heads,self.head_size)                # batch x seq x heads x head_size\n",
    "        Q = np.transpose(Q,(0,2,1,3))                                                 # batch x heads x seq x head_size\n",
    "        \n",
    "        K = np.dot(K,self.WK).reshape(K.shape[0],K.shape[1],self.heads,self.dim_size) # batch x seq x heads x head_size\n",
    "        K = np.transpose(K,(0,2,1,3))                                                 # batch x heads x seq x head_size\n",
    "        \n",
    "        V = np.dot(V,self.WV).reshape(V.shape[0],V.shape[1],self.heads,self.dim_size) # batch x seq x heads x head_size\n",
    "        V = np.transpose(V,(0,2,1,3))                                                 # batch x heads x seq x head_size\n",
    "        \n",
    "        values = self.scaled_dot_product(Q,K,V,Mask)                                  # batch x heads x seq x head_size\n",
    "        values = np.transpose(values,(0,2,1,3))                                       # batch x seq x heads x head_size\n",
    "        values = values.reshape(value.shape[0],values.shape[1],self.emb_size)         # batch x seq x dim_size\n",
    "        out = np.dot(values,self.WO)                                                  # batch x seq x dim_size\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,in_features,out_features):\n",
    "        scale = 1/np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-scale,scale,(in_features,out_features))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, dim_size, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        # declare your MHA layer\n",
    "        # weights for Feedforward layer\n",
    "        self.self_attn = MultiHeadAttention(dim_size,num_heads)\n",
    "        self.linear1 = Linear(in_features=dim_size,out_features=dim_size*2)\n",
    "        self.activation = Relu()\n",
    "        self.linear2 = Linear(in_features = dim_size*2, out_features = dim_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ## MHA\n",
    "        attn_out = self.self_attn(X,X,X,None)\n",
    "        \n",
    "        ## Add\n",
    "        X = X + attn_out\n",
    "        \n",
    "        ## Feedforward\n",
    "        x1 = self.linear1(X)\n",
    "        x1 = self.activation(x1)\n",
    "        x1 = self.linear1(x1)\n",
    "        \n",
    "        ## Add\n",
    "        X = X + x1\n",
    "        \n",
    "        return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, dim_size, num_heads1, num_heads2):\n",
    "        # declare your MHA layer\n",
    "        # weights for Feedforward layer\n",
    "        self.self_masked_attn = MultiHeadAttention(dim_size,num_heads1)\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(dim_size,num_heads2)\n",
    "        self.linear1 = Linear(in_features=dim_size,out_features=dim_size*2)\n",
    "        self.activation = Relu()\n",
    "        self.linear2 = Linear(in_features = dim_size*2, out_features = dim_size)\n",
    "        \n",
    "    def forward(self, X, encoder_input, mask):\n",
    "        ## Masked MHA\n",
    "        mask_attn_out = self.self_masked_attn(X,X,X,mask)\n",
    "        X = X + mask_attn_out \n",
    "        \n",
    "        ## MHA\n",
    "        attn_out = self.self_attn(encoder_input,encoder_input,X,None)\n",
    "        \n",
    "        ## Add\n",
    "        X = X + attn_out\n",
    "        \n",
    "        ## Feedforward\n",
    "        x1 = self.linear1(X)\n",
    "        x1 = self.activation(x1)\n",
    "        x1 = self.linear1(x1)\n",
    "        \n",
    "        ## Add\n",
    "        X = X + x1\n",
    "        \n",
    "        return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
