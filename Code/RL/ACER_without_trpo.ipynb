{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCKap9nNodOw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SA0CDifItbZ"
      },
      "source": [
        "### Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEOTTPlTKllj",
        "outputId": "a239d2ef-1f77-4347-e947-82aed17a50b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 19.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wsNO3FuyIuOZ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Discrete as DiscreteSpace\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import multiprocessing as mp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEwiIU77IbFw"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fRFg13-oIWIf"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'next_states',\n",
        "                                       'done', 'exploration_statistics'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay buffer for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.episodes = deque([[]], maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    def add(self, transition):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            The transition to add.\n",
        "        \"\"\"\n",
        "        if self.episodes[-1] and self.episodes[-1][-1].done[0, 0]:\n",
        "            self.episodes.append([])\n",
        "        self.episodes[-1].append(transition)\n",
        "\n",
        "    def sample(self, batch_size, window_length=float('inf')):\n",
        "        \"\"\"\n",
        "        Sample a batch of trajectories from the buffer. If they are of unequal length\n",
        "        (which is likely), the trajectories will be padded with zero-reward transitions.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            The batch size of the sample.\n",
        "        window_length : int, optional\n",
        "            The window length.\n",
        "        Returns\n",
        "        -------\n",
        "        list of Transition's\n",
        "            A batched sampled trajectory.\n",
        "        \"\"\"\n",
        "        batched_trajectory = []\n",
        "        trajectory_indices = random.choices(range(len(self.episodes)-1), k=min(batch_size, len(self.episodes)-1))\n",
        "        trajectories = []\n",
        "        for trajectory in [self.episodes[index] for index in trajectory_indices]:\n",
        "            start = random.choices(range(len(trajectory)), k=1)[0]\n",
        "            trajectories.append(trajectory[start:start + window_length])\n",
        "        smallest_trajectory_length = min([len(trajectory) for trajectory in trajectories]) if trajectories else 0\n",
        "        for index in range(len(trajectories)):\n",
        "            trajectories[index] = trajectories[index][-smallest_trajectory_length:]\n",
        "        for transitions in zip(*trajectories):\n",
        "            batched_transition = Transition(*[torch.cat(data, dim=0) for data in zip(*transitions)])\n",
        "            batched_trajectory.append(batched_transition)\n",
        "        return batched_trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def extend(transition):\n",
        "        \"\"\"\n",
        "        Generate a new zero-reward transition to extend a trajectory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            A terminal transition which will become the new transition's previous\n",
        "            transition in the trajectory.\n",
        "        Returns\n",
        "        -------\n",
        "        Transition\n",
        "            The new transition that can be used to extend a trajectory.\n",
        "        \"\"\"\n",
        "        if not transition.done[0, 0]:\n",
        "            raise ValueError(\"Can only extend a terminal transition.\")\n",
        "        exploration_statistics = torch.ones(transition.exploration_statistics.size()) \\\n",
        "                                 / transition.exploration_statistics.size(-1)\n",
        "        transition = Transition(states=transition.next_states,\n",
        "                                actions=transition.actions,\n",
        "                                rewards=torch.FloatTensor([[0.]]),\n",
        "                                next_states=transition.next_states,\n",
        "                                done=transition.done,\n",
        "                                exploration_statistics=exploration_statistics)\n",
        "        return transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXtKkquIg_v"
      },
      "source": [
        "### Ornstein Uhlenbeck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "srY32EPTImkw"
      },
      "outputs": [],
      "source": [
        "class OrnsteinUhlenbeckProcess:\n",
        "    def __init__(self, theta, mu, sigma, time_scale=1e-1,\n",
        "                 size=1, initial_value=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.time_scale = time_scale\n",
        "        self.size = size\n",
        "        self.initial_value = initial_value if initial_value is not None else np.zeros(size)\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sample(self):\n",
        "        value = self.previous_value\n",
        "        value += self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        value += self.sigma * np.sqrt(self.time_scale) * np.random.normal(size=self.size)\n",
        "        return value\n",
        "\n",
        "    def reset(self):\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sampling_parameters(self):\n",
        "        mean = self.previous_value + self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        sd = self.sigma * np.sqrt(self.time_scale) * np.ones((self.size,))\n",
        "        return mean, sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsR1jS81aVu0"
      },
      "source": [
        "### Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2gGuX7mCIzm6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, *input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def copy_parameters_from(self, source, decay=0.):\n",
        "        \"\"\"\n",
        "        Copy the parameters from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the parameters.\n",
        "        decay : float, optional\n",
        "            How much decay should be applied? Default is 0., which means the parameters\n",
        "            are completely copied.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter.data.copy_(decay * parameter.data + (1 - decay) * source_parameter.data)\n",
        "\n",
        "    def copy_gradients_from(self, source):\n",
        "        \"\"\"\n",
        "        Copy the gradients from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the gradients.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter._grad = source_parameter.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igZXsvZ6aNaY"
      },
      "source": [
        "##### Discrete Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UCeJLhvxaMn8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiscreteActorCritic(ActorCritic):\n",
        "    \"\"\"\n",
        "    Discrete actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_layer = torch.nn.Linear(STATE_SPACE_DIM, 32)\n",
        "        self.hidden_layer = torch.nn.Linear(32, 32)\n",
        "        self.action_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "        self.action_value_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        Compute a forward pass in the network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        states : torch.Tensor\n",
        "            The states for which the action probabilities and the action-values must be computed.\n",
        "        Returns\n",
        "        -------\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action probabilities of the policy according to the actor.\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action-values of the policy according to the critic.\n",
        "        \"\"\"\n",
        "        hidden = F.relu(self.input_layer(states))\n",
        "        hidden = F.relu(self.hidden_layer(hidden))\n",
        "        action_probabilities = F.softmax(self.action_layer(hidden), dim=-1)\n",
        "        action_values = self.action_value_layer(hidden)\n",
        "        return action_probabilities, action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_N8FWAlacq_"
      },
      "source": [
        "### Brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WQWMtWLZTFNL"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Brain:\n",
        "    \"\"\"\n",
        "    A centralized brain for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.actor_critic = None\n",
        "        self.average_actor_critic = None\n",
        "        self.train_logs = mp.Queue()\n",
        "\n",
        "class DiscreteBrain(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.actor_critic = DiscreteActorCritic()\n",
        "        self.actor_critic.share_memory()\n",
        "        self.average_actor_critic = DiscreteActorCritic()\n",
        "        self.average_actor_critic.share_memory()\n",
        "        self.average_actor_critic.copy_parameters_from(self.actor_critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxotsgUI9Qk"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iOX6bbPBI-Ps"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent that learns an optimal policy using ACER.\n",
        "    Parameters\n",
        "    ----------\n",
        "    brain : brain.Brain\n",
        "        The brain to update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    def __init__(self, brain, render=False, verbose=False):\n",
        "        self.env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "        self.env.reset()\n",
        "        self.render = render\n",
        "        self.verbose = verbose\n",
        "        self.buffer = ReplayBuffer()\n",
        "        self.brain = brain\n",
        "        self.optimizer = torch.optim.Adam(brain.actor_critic.parameters(),\n",
        "                                          lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pv9UAMzbLhZ"
      },
      "source": [
        "#### Discrete Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xMxHxq-NbEEf"
      },
      "outputs": [],
      "source": [
        "class DiscreteAgent(Agent):\n",
        "    def __init__(self, brain, render=True, verbose=True):\n",
        "        super().__init__(brain, render, verbose)\n",
        "        \n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the agent for several episodes.\n",
        "        \"\"\"\n",
        "        for episode in range(MAX_EPISODES):\n",
        "            episode_rewards = 0\n",
        "            episode_length = 0\n",
        "            end_of_episode = False\n",
        "#             if self.verbose:\n",
        "#                 print(\"Episode #{}\".format(episode), end=\"\")\n",
        "            while not end_of_episode:\n",
        "                trajectory = self.explore(self.brain.actor_critic) \n",
        "                self.learning_iteration(trajectory)\n",
        "                end_of_episode = trajectory[-1].done[0, 0]\n",
        "                episode_rewards += sum([transition.rewards[0, 0] for transition in trajectory])\n",
        "                episode_length += len(trajectory)\n",
        "                for trajectory_count in range(np.random.poisson(REPLAY_RATIO)):\n",
        "                    trajectory = self.buffer.sample(OFF_POLICY_MINIBATCH_SIZE, MAX_REPLAY_SIZE)\n",
        "                    if trajectory:\n",
        "                        self.learning_iteration(trajectory)\n",
        "                        episode_length += len(trajectory)\n",
        "\n",
        "#             if self.verbose:\n",
        "#                 print(\", episode rewards {} - episode length {}\".format(episode_rewards,episode_length))\n",
        "            self.brain.train_logs.put([episode_rewards.numpy(),episode_length,episode])\n",
        "\n",
        "             \n",
        "    def learning_iteration(self, trajectory):\n",
        "        \"\"\"\n",
        "        Conduct a single discrete learning iteration. Analogue of Algorithm 2 in the paper.\n",
        "        \"\"\"\n",
        "        actor_critic = DiscreteActorCritic()\n",
        "        actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "\n",
        "        _, _, _, next_states, _, _ = trajectory[-1]\n",
        "        action_probabilities, action_values = actor_critic(Variable(next_states))\n",
        "        retrace_action_value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1)\n",
        "\n",
        "        for states, actions, rewards, _, done, exploration_probabilities in reversed(trajectory):\n",
        "            action_probabilities, action_values = actor_critic(Variable(states))\n",
        "            average_action_probabilities, _ = self.brain.average_actor_critic(Variable(states))\n",
        "            value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1) * (1. - done)\n",
        "            action_indices = Variable(actions.long())\n",
        "\n",
        "            importance_weights = action_probabilities.data / exploration_probabilities\n",
        "            \n",
        "            naive_advantage = action_values.gather(-1, action_indices).data - value\n",
        "            retrace_action_value = rewards + DISCOUNT_FACTOR * retrace_action_value * (1. - done)\n",
        "            retrace_advantage = retrace_action_value - value\n",
        "\n",
        "            # Actor\n",
        "            actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "                importance_weights.gather(-1, action_indices.data).clamp(max=TRUNCATION_PARAMETER) * retrace_advantage) \\\n",
        "                * action_probabilities.gather(-1, action_indices).log()\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "            #                           naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(importance_weights.clamp(min=0.) *\n",
        "            #                           naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            bias_correction = - ACTOR_LOSS_WEIGHT * Variable((1 - TRUNCATION_PARAMETER / importance_weights).clamp(min=0.) *\n",
        "                                      naive_advantage * action_probabilities.data) * action_probabilities.log()\n",
        "            actor_loss += bias_correction.sum(-1).unsqueeze(-1)\n",
        "            actor_gradients = torch.autograd.grad(actor_loss.mean(), action_probabilities, retain_graph=True)\n",
        "#             actor_gradients = self.discrete_trust_region_update(actor_gradients, action_probabilities,\n",
        "#                                                        Variable(average_action_probabilities.data))\n",
        "            action_probabilities.backward(actor_gradients, retain_graph=True)\n",
        "\n",
        "            # Critic\n",
        "            critic_loss = (action_values.gather(-1, action_indices) - Variable(retrace_action_value)).pow(2)\n",
        "            # critic_loss = (action_values.gather(-1, action_indices) - Variable(value)).pow(2)\n",
        "            critic_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            # Entropy\n",
        "            entropy_loss = ENTROPY_REGULARIZATION * (action_probabilities * action_probabilities.log()).sum(-1)\n",
        "            entropy_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            retrace_action_value = importance_weights.gather(-1, action_indices.data).clamp(max=1.) * \\\n",
        "                                   (retrace_action_value - action_values.gather(-1, action_indices).data) + value\n",
        "        self.brain.actor_critic.copy_gradients_from(actor_critic)\n",
        "        self.optimizer.step()\n",
        "#         self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic, decay=TRUST_REGION_DECAY)\n",
        "        self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "        \n",
        "\n",
        "    def explore(self, actor_critic):\n",
        "        \"\"\"\n",
        "        Explore an environment by taking a sequence of actions and saving the results in the memory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_critic : ActorCritic\n",
        "            The actor-critic model to use to explore.\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(self.env.env.state)\n",
        "        trajectory = []\n",
        "        for step in range(MAX_STEPS_BEFORE_UPDATE):\n",
        "            action_probabilities, *_ = actor_critic(Variable(state))\n",
        "            action = action_probabilities.multinomial(1)\n",
        "            action = action.data\n",
        "            exploration_statistics = action_probabilities.data.view(1, -1)\n",
        "            next_state, reward, done, _= self.env.step(action.numpy()[0])\n",
        "            next_state = torch.from_numpy(next_state).float()\n",
        "            if self.render:\n",
        "                self.env.render()\n",
        "            transition = Transition(states=state.view(1, -1),\n",
        "                                                  actions=action.view(1, -1),\n",
        "                                                  rewards=torch.FloatTensor([[reward]]),\n",
        "                                                  next_states=next_state.view(1, -1),\n",
        "                                                  done=torch.FloatTensor([[done]]),\n",
        "                                                  exploration_statistics=exploration_statistics)\n",
        "            self.buffer.add(transition)\n",
        "            trajectory.append(transition)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "                break\n",
        "            else:\n",
        "                state = next_state\n",
        "        return trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def discrete_trust_region_update(actor_gradients, action_probabilities, average_action_probabilities):\n",
        "        \"\"\"\n",
        "        Update the actor gradients so that they satisfy a linearized KL constraint with respect\n",
        "        to the average actor-critic network. See Section 3.3 of the paper for details.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_gradients : tuple of torch.Tensor's\n",
        "            The original gradients.\n",
        "        action_probabilities\n",
        "            The action probabilities according to the current actor-critic network.\n",
        "        average_action_probabilities\n",
        "            The action probabilities according to the average actor-critic network.\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of torch.Tensor's\n",
        "            The updated gradients.\n",
        "        \"\"\"\n",
        "        negative_kullback_leibler = - ((average_action_probabilities.log() - action_probabilities.log())\n",
        "                                       * average_action_probabilities).sum(-1)\n",
        "        kullback_leibler_gradients = torch.autograd.grad(negative_kullback_leibler.mean(),\n",
        "                                                         action_probabilities, retain_graph=True)\n",
        "        updated_actor_gradients = []\n",
        "        for actor_gradient, kullback_leibler_gradient in zip(actor_gradients, kullback_leibler_gradients):\n",
        "            scale = actor_gradient.mul(kullback_leibler_gradient).sum(-1).unsqueeze(-1) - TRUST_REGION_CONSTRAINT\n",
        "            scale = torch.div(scale, actor_gradient.mul(actor_gradient).sum(-1).unsqueeze(-1)).clamp(min=0.)\n",
        "            updated_actor_gradients.append(actor_gradient - scale * kullback_leibler_gradient)\n",
        "        return updated_actor_gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMiAqHrI_7l"
      },
      "source": [
        "### Run agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kltwuUSnJEt2"
      },
      "outputs": [],
      "source": [
        "def run_agent(shared_brain, render=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Run the agent.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shared_brain : brain.Brain\n",
        "        The shared brain the agents will use and update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    local_agent = DiscreteAgent(shared_brain, render, verbose)\n",
        "    local_agent.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(ENVIRONMENT_NAME=\"CartPole-v1\",model_path=\"model.pkl\",num_episodes=1000):\n",
        "  \n",
        "  # Create the environment\n",
        "  env = gym.make(ENVIRONMENT_NAME)\n",
        "  \n",
        "  ## Environment parameters\n",
        "  action_space = env.action_space\n",
        "  state_space = env.observation_space\n",
        "  ACTION_SPACE_DIM = action_space.n\n",
        "  STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "\n",
        "  ## load model here\n",
        "  model = torch.load(model_path)\n",
        "  model.eval()\n",
        "\n",
        "  # Set the number of episodes to run\n",
        "  num_episodes = 1000\n",
        "\n",
        "  # Create lists to store the returns for each episode\n",
        "  returns = []\n",
        "\n",
        "  # Run the episodes\n",
        "  for episode in range(num_episodes):\n",
        "    # Reset the environment at the start of each episode\n",
        "    state = env.reset()\n",
        "    \n",
        "    # Initialize the episode return\n",
        "    episode_return = 0\n",
        "    episode_length = 0  \n",
        "    # Run the episode\n",
        "    done = False\n",
        "    while not done:\n",
        "      # Get the action from the model\n",
        "      action_prob,_  = model(torch.Tensor(state))\n",
        "      \n",
        "      # Step the environment with the action\n",
        "      next_state, reward, done, _ = env.step(torch.argmax(action_prob).numpy())\n",
        "      \n",
        "      # Update the episode return\n",
        "      episode_return += reward\n",
        "      episode_length += 1\n",
        "      # Update the state\n",
        "      state = next_state\n",
        "    # Add the episode return to the list of returns\n",
        "    returns.append([episode_return,episode_length]) \n",
        "#     print(episode,episode_return,episode_length)\n",
        "  return returns"
      ],
      "metadata": {
        "id": "vDvqXBjhlG8k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwJe8DyVblIM"
      },
      "source": [
        "### Init Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LlLEkzf2bq-9"
      },
      "outputs": [],
      "source": [
        "\n",
        "ENVIRONMENT_NAME = 'CartPole-v1'\n",
        "# ENVIRONMENT_NAME = 'MountainCarContinuous-v0'\n",
        "\n",
        "env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "action_space = env.action_space\n",
        "state_space = env.observation_space\n",
        "env.close()\n",
        "del env\n",
        "\n",
        "ACTION_SPACE_DIM = action_space.n\n",
        "CONTROL = 'discrete'\n",
        "STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "# Parameters that work well for CartPole-v0\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "### this is hyperparameter\n",
        "REPLAY_BUFFER_SIZE = 100\n",
        "REPLAY_RATIO = 4\n",
        "MAX_EPISODES = 500\n",
        "OFF_POLICY_MINIBATCH_SIZE = 16\n",
        "MAX_REPLAY_SIZE = 200\n",
        "\n",
        "\n",
        "TRUNCATION_PARAMETER = 10\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "MAX_STEPS_BEFORE_UPDATE = 20\n",
        "NUMBER_OF_AGENTS = 2\n",
        "TRUST_REGION_CONSTRAINT = 1.\n",
        "TRUST_REGION_DECAY = 0.99\n",
        "ENTROPY_REGULARIZATION = 1e-3\n",
        "ACTOR_LOSS_WEIGHT = 0.1\n",
        "\n",
        "# Not used for discrete agents\n",
        "ORNSTEIN_UHLENBECK_NOISE_SCALE = None\n",
        "INITIAL_ORNSTEIN_UHLENBECK_NOISE_RATIO = None\n",
        "NUMBER_OF_EXPLORATION_EPISODES = None\n",
        "INITIAL_STANDARD_DEVIATION = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP0YfFDCbtTe"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir retracebias1"
      ],
      "metadata": {
        "id": "mQ18bOEvlRoI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cu4ilTKobWZ9",
        "outputId": "15894a1f-f506-46ef-80c6-84c23a6676f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_1_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_2_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_4_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_100_8_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_1_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_2_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_6_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_7_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_8_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_4_9_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_0_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_1_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_2_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_3_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_4_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_5_retracebias_\n",
            "############################################################\n",
            "retracebias1/CartPole-v1_250_8_6_retracebias_\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d401a53f082c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mlocal_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mlocal_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;31m# length = brain.train_logs.qsize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-7b691d7a2106>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOFF_POLICY_MINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_REPLAY_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                         \u001b[0mepisode_length\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-7b691d7a2106>\u001b[0m in \u001b[0;36mlearning_iteration\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrace_action_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# critic_loss = (action_values.gather(-1, action_indices) - Variable(value)).pow(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for MAX_EPISODES in [100,250,500]:\n",
        "    for REPLAY_RATIO in [1,2,4,8]:\n",
        "            for itern in range(10):\n",
        "#                 try:\n",
        "                brain = DiscreteBrain()\n",
        "                local_agent = DiscreteAgent(brain, False,True)\n",
        "                local_agent.run()\n",
        "\n",
        "                # length = brain.train_logs.qsize()\n",
        "                # out = [brain.train_logs.get_nowait() for _ in range(length)]\n",
        "                # train_result = pd.DataFrame(out,columns=[\"Expected Reward\",\"Episode length\",\"Episode number\"])\n",
        "\n",
        "                pathName = \"retracebias1/\" + ENVIRONMENT_NAME + \"_\" + str(MAX_EPISODES) + \"_\" + str(REPLAY_RATIO) \\\n",
        "                            + \"_\" + str(itern) + \"_\" + \"retracebias_\"\n",
        "                print(\"############################################################\")\n",
        "                print(pathName)\n",
        "                ## save logs\n",
        "                train_result.to_csv( pathName + \"train_result.csv\",index=None)\n",
        "\n",
        "                ## save model\n",
        "                torch.save(brain.actor_critic, pathName + \"model.pkl\")\n",
        "\n",
        "                returns = test(ENVIRONMENT_NAME, model_path=  pathName+\"model.pkl\", num_episodes=1000)\n",
        "\n",
        "                test_results = pd.DataFrame(returns,columns=[\"Expected Rewards\",\"Episode Length\"])\n",
        "                test_results.to_csv(pathName+ \"test_result.csv\",index=None)\n",
        "#                 except:\n",
        "#                     print(\"-------------------------------ERROR ------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "250,2,1"
      ],
      "metadata": {
        "id": "ovYtxLWHR8Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "M1qweDGocg9A"
      },
      "outputs": [],
      "source": [
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out =[]\n",
        "for file in glob.glob(\"./retracebias1//*test*.csv\"):\n",
        "    data = pd.read_csv(file)\n",
        "    out.append(file.split(\"_\")[1:4] + [data[\"Expected Rewards\"].mean(),data['Expected Rewards'].std()])\n"
      ],
      "metadata": {
        "id": "1U2MDOxx5ILJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resdf = pd.DataFrame(out,columns=[\"Max_episodes\",\"Replay\",\"itern\",\"mean\",\"std\"]).groupby(by=[\"Max_episodes\",\"Replay\"],as_index=False).mean()"
      ],
      "metadata": {
        "id": "LSUfGt6N5cPY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "4L1i0PPvEZJL",
        "outputId": "42ffce08-00d9-417c-e758-06ef4288d6ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Max_episodes Replay        mean        std\n",
              "0          100      1   25.473500  10.772646\n",
              "1          100      2   39.483500  15.674377\n",
              "2          100      4   23.927100   3.428440\n",
              "3          100      8   74.914400  20.848302\n",
              "4          250      1   77.937600  12.494724\n",
              "5          250      2  258.180600  39.358356\n",
              "6          250      4  162.531300  10.270949\n",
              "7          250      8  120.614857   5.154256"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c76af3a8-c47e-45fe-8c52-09b50f6613a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_episodes</th>\n",
              "      <th>Replay</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>25.473500</td>\n",
              "      <td>10.772646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>39.483500</td>\n",
              "      <td>15.674377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>23.927100</td>\n",
              "      <td>3.428440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>8</td>\n",
              "      <td>74.914400</td>\n",
              "      <td>20.848302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>250</td>\n",
              "      <td>1</td>\n",
              "      <td>77.937600</td>\n",
              "      <td>12.494724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>258.180600</td>\n",
              "      <td>39.358356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>250</td>\n",
              "      <td>4</td>\n",
              "      <td>162.531300</td>\n",
              "      <td>10.270949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>120.614857</td>\n",
              "      <td>5.154256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c76af3a8-c47e-45fe-8c52-09b50f6613a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c76af3a8-c47e-45fe-8c52-09b50f6613a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c76af3a8-c47e-45fe-8c52-09b50f6613a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in [\"100\",\"250\"]:\n",
        "    plt.errorbar(resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"Replay\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"mean\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"std\"],label=\"Max episodes = \"+ episode)\n",
        "plt.xlabel(\"REPLAY RATIO\")\n",
        "plt.ylabel(\"Average Reward with std deviation\")\n",
        "plt.title(\"Max episodes for training vs Replay Ratio\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "vsA2wmpJEaV3",
        "outputId": "628d021d-c10b-4d4d-cf18-c8440fbc5118"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa06addd8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5C5DwZFEbxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}