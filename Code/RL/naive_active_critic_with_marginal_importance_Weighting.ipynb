{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDvPgv65H_B-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tw4dti2pfwK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SA0CDifItbZ"
      },
      "source": [
        "### Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEOTTPlTKllj",
        "outputId": "0e7b1c27-89dd-4c94-b8c4-bd4d891d32cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wsNO3FuyIuOZ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Discrete as DiscreteSpace\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import multiprocessing as mp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEwiIU77IbFw"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fRFg13-oIWIf"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'next_states',\n",
        "                                       'done', 'exploration_statistics'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay buffer for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.episodes = deque([[]], maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    def add(self, transition):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            The transition to add.\n",
        "        \"\"\"\n",
        "        if self.episodes[-1] and self.episodes[-1][-1].done[0, 0]:\n",
        "            self.episodes.append([])\n",
        "        self.episodes[-1].append(transition)\n",
        "\n",
        "    def sample(self, batch_size, window_length=float('inf')):\n",
        "        \"\"\"\n",
        "        Sample a batch of trajectories from the buffer. If they are of unequal length\n",
        "        (which is likely), the trajectories will be padded with zero-reward transitions.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            The batch size of the sample.\n",
        "        window_length : int, optional\n",
        "            The window length.\n",
        "        Returns\n",
        "        -------\n",
        "        list of Transition's\n",
        "            A batched sampled trajectory.\n",
        "        \"\"\"\n",
        "        batched_trajectory = []\n",
        "        trajectory_indices = random.choices(range(len(self.episodes)-1), k=min(batch_size, len(self.episodes)-1))\n",
        "        trajectories = []\n",
        "        for trajectory in [self.episodes[index] for index in trajectory_indices]:\n",
        "            start = random.choices(range(len(trajectory)), k=1)[0]\n",
        "            trajectories.append(trajectory[start:start + window_length])\n",
        "        smallest_trajectory_length = min([len(trajectory) for trajectory in trajectories]) if trajectories else 0\n",
        "        for index in range(len(trajectories)):\n",
        "            trajectories[index] = trajectories[index][-smallest_trajectory_length:]\n",
        "        for transitions in zip(*trajectories):\n",
        "            batched_transition = Transition(*[torch.cat(data, dim=0) for data in zip(*transitions)])\n",
        "            batched_trajectory.append(batched_transition)\n",
        "        return batched_trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def extend(transition):\n",
        "        \"\"\"\n",
        "        Generate a new zero-reward transition to extend a trajectory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            A terminal transition which will become the new transition's previous\n",
        "            transition in the trajectory.\n",
        "        Returns\n",
        "        -------\n",
        "        Transition\n",
        "            The new transition that can be used to extend a trajectory.\n",
        "        \"\"\"\n",
        "        if not transition.done[0, 0]:\n",
        "            raise ValueError(\"Can only extend a terminal transition.\")\n",
        "        exploration_statistics = torch.ones(transition.exploration_statistics.size()) \\\n",
        "                                 / transition.exploration_statistics.size(-1)\n",
        "        transition = Transition(states=transition.next_states,\n",
        "                                actions=transition.actions,\n",
        "                                rewards=torch.FloatTensor([[0.]]),\n",
        "                                next_states=transition.next_states,\n",
        "                                done=transition.done,\n",
        "                                exploration_statistics=exploration_statistics)\n",
        "        return transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXtKkquIg_v"
      },
      "source": [
        "### Ornstein Uhlenbeck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "srY32EPTImkw"
      },
      "outputs": [],
      "source": [
        "class OrnsteinUhlenbeckProcess:\n",
        "    def __init__(self, theta, mu, sigma, time_scale=1e-1,\n",
        "                 size=1, initial_value=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.time_scale = time_scale\n",
        "        self.size = size\n",
        "        self.initial_value = initial_value if initial_value is not None else np.zeros(size)\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sample(self):\n",
        "        value = self.previous_value\n",
        "        value += self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        value += self.sigma * np.sqrt(self.time_scale) * np.random.normal(size=self.size)\n",
        "        return value\n",
        "\n",
        "    def reset(self):\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sampling_parameters(self):\n",
        "        mean = self.previous_value + self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        sd = self.sigma * np.sqrt(self.time_scale) * np.ones((self.size,))\n",
        "        return mean, sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsR1jS81aVu0"
      },
      "source": [
        "### Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2gGuX7mCIzm6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, *input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def copy_parameters_from(self, source, decay=0.):\n",
        "        \"\"\"\n",
        "        Copy the parameters from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the parameters.\n",
        "        decay : float, optional\n",
        "            How much decay should be applied? Default is 0., which means the parameters\n",
        "            are completely copied.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter.data.copy_(decay * parameter.data + (1 - decay) * source_parameter.data)\n",
        "\n",
        "    def copy_gradients_from(self, source):\n",
        "        \"\"\"\n",
        "        Copy the gradients from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the gradients.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter._grad = source_parameter.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igZXsvZ6aNaY"
      },
      "source": [
        "##### Discrete Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UCeJLhvxaMn8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiscreteActorCritic(ActorCritic):\n",
        "    \"\"\"\n",
        "    Discrete actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_layer = torch.nn.Linear(STATE_SPACE_DIM, 32)\n",
        "        self.hidden_layer = torch.nn.Linear(32, 32)\n",
        "        self.action_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "        self.action_value_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        Compute a forward pass in the network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        states : torch.Tensor\n",
        "            The states for which the action probabilities and the action-values must be computed.\n",
        "        Returns\n",
        "        -------\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action probabilities of the policy according to the actor.\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action-values of the policy according to the critic.\n",
        "        \"\"\"\n",
        "        hidden = F.relu(self.input_layer(states))\n",
        "        hidden = F.relu(self.hidden_layer(hidden))\n",
        "        action_probabilities = F.softmax(self.action_layer(hidden), dim=-1)\n",
        "        action_values = self.action_value_layer(hidden)\n",
        "        return action_probabilities, action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_N8FWAlacq_"
      },
      "source": [
        "### Brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WQWMtWLZTFNL"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Brain:\n",
        "    \"\"\"\n",
        "    A centralized brain for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.actor_critic = None\n",
        "        self.average_actor_critic = None\n",
        "        self.train_logs = mp.Queue()\n",
        "\n",
        "class DiscreteBrain(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.actor_critic = DiscreteActorCritic()\n",
        "        self.actor_critic.share_memory()\n",
        "        self.average_actor_critic = DiscreteActorCritic()\n",
        "        self.average_actor_critic.share_memory()\n",
        "        self.average_actor_critic.copy_parameters_from(self.actor_critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxotsgUI9Qk"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iOX6bbPBI-Ps"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent that learns an optimal policy using ACER.\n",
        "    Parameters\n",
        "    ----------\n",
        "    brain : brain.Brain\n",
        "        The brain to update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    def __init__(self, brain, render=False, verbose=False):\n",
        "        self.env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "        self.env.reset()\n",
        "        self.render = render\n",
        "        self.verbose = verbose\n",
        "        self.buffer = ReplayBuffer()\n",
        "        self.brain = brain\n",
        "        self.optimizer = torch.optim.Adam(brain.actor_critic.parameters(),\n",
        "                                          lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pv9UAMzbLhZ"
      },
      "source": [
        "#### Discrete Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xMxHxq-NbEEf"
      },
      "outputs": [],
      "source": [
        "class DiscreteAgent(Agent):\n",
        "    def __init__(self, brain, render=True, verbose=True):\n",
        "        super().__init__(brain, render, verbose)\n",
        "        \n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the agent for several episodes.\n",
        "        \"\"\"\n",
        "        for episode in range(MAX_EPISODES):\n",
        "            episode_rewards = 0\n",
        "            episode_length = 0\n",
        "            end_of_episode = False\n",
        "#             if self.verbose:\n",
        "#                 print(\"Episode #{}\".format(episode), end=\"\")\n",
        "            while not end_of_episode:\n",
        "                trajectory = self.explore(self.brain.actor_critic) \n",
        "                self.learning_iteration(trajectory)\n",
        "                end_of_episode = trajectory[-1].done[0, 0]\n",
        "                episode_rewards += sum([transition.rewards[0, 0] for transition in trajectory])\n",
        "                episode_length += len(trajectory)\n",
        "                for trajectory_count in range(np.random.poisson(REPLAY_RATIO)):\n",
        "                    trajectory = self.buffer.sample(OFF_POLICY_MINIBATCH_SIZE, MAX_REPLAY_SIZE)\n",
        "                    if trajectory:\n",
        "                        self.learning_iteration(trajectory)\n",
        "                        episode_length += len(trajectory)\n",
        "\n",
        "#             if self.verbose:\n",
        "#                 print(\", episode rewards {} - episode length {}\".format(episode_rewards,episode_length))\n",
        "            self.brain.train_logs.put([episode_rewards.numpy(),episode_length,episode])\n",
        "\n",
        "             \n",
        "    def learning_iteration(self, trajectory):\n",
        "        \"\"\"\n",
        "        Conduct a single discrete learning iteration. Analogue of Algorithm 2 in the paper.\n",
        "        \"\"\"\n",
        "        actor_critic = DiscreteActorCritic()\n",
        "        actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "\n",
        "        _, _, _, next_states, _, _ = trajectory[-1]\n",
        "        action_probabilities, action_values = actor_critic(Variable(next_states))\n",
        "        retrace_action_value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1)\n",
        "\n",
        "        for states, actions, rewards, _, done, exploration_probabilities in reversed(trajectory):\n",
        "            action_probabilities, action_values = actor_critic(Variable(states))\n",
        "            average_action_probabilities, _ = self.brain.average_actor_critic(Variable(states))\n",
        "            value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1) * (1. - done)\n",
        "            action_indices = Variable(actions.long())\n",
        "\n",
        "            importance_weights = action_probabilities.data / exploration_probabilities\n",
        "            \n",
        "            naive_advantage = action_values.gather(-1, action_indices).data - value\n",
        "            retrace_action_value = rewards + DISCOUNT_FACTOR * retrace_action_value * (1. - done)\n",
        "            retrace_advantage = retrace_action_value - value\n",
        "\n",
        "            # Actor\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "            #     importance_weights.gather(-1, action_indices.data).clamp(max=TRUNCATION_PARAMETER) * retrace_advantage) \\\n",
        "            #     * action_probabilities.gather(-1, action_indices).log()\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "            #                           naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            actor_loss = - ACTOR_LOSS_WEIGHT * Variable(importance_weights.clamp(min=0.) *\n",
        "                                      naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            # bias_correction = - ACTOR_LOSS_WEIGHT * Variable((1 - TRUNCATION_PARAMETER / importance_weights).clamp(min=0.) *\n",
        "            #                           naive_advantage * action_probabilities.data) * action_probabilities.log()\n",
        "            # actor_loss += bias_correction.sum(-1).unsqueeze(-1)\n",
        "            actor_gradients = torch.autograd.grad(actor_loss.mean(), action_probabilities, retain_graph=True)\n",
        "            # actor_gradients = self.discrete_trust_region_update(actor_gradients, action_probabilities,\n",
        "            #                                            Variable(average_action_probabilities.data))\n",
        "            action_probabilities.backward(actor_gradients, retain_graph=True)\n",
        "\n",
        "            # Critic\n",
        "            # critic_loss = (action_values.gather(-1, action_indices) - Variable(retrace_action_value)).pow(2)\n",
        "            critic_loss = (action_values.gather(-1, action_indices) - Variable(value)).pow(2)\n",
        "            critic_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            # Entropy\n",
        "            entropy_loss = ENTROPY_REGULARIZATION * (action_probabilities * action_probabilities.log()).sum(-1)\n",
        "            entropy_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            retrace_action_value = importance_weights.gather(-1, action_indices.data).clamp(max=1.) * \\\n",
        "                                   (retrace_action_value - action_values.gather(-1, action_indices).data) + value\n",
        "        self.brain.actor_critic.copy_gradients_from(actor_critic)\n",
        "        self.optimizer.step()\n",
        "        # self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic, decay=TRUST_REGION_DECAY)\n",
        "        self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "        \n",
        "\n",
        "    def explore(self, actor_critic):\n",
        "        \"\"\"\n",
        "        Explore an environment by taking a sequence of actions and saving the results in the memory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_critic : ActorCritic\n",
        "            The actor-critic model to use to explore.\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(self.env.env.state)\n",
        "        trajectory = []\n",
        "        for step in range(MAX_STEPS_BEFORE_UPDATE):\n",
        "            action_probabilities, *_ = actor_critic(Variable(state))\n",
        "            action = action_probabilities.multinomial(1)\n",
        "            action = action.data\n",
        "            exploration_statistics = action_probabilities.data.view(1, -1)\n",
        "            next_state, reward, done, _= self.env.step(action.numpy()[0])\n",
        "            next_state = torch.from_numpy(next_state).float()\n",
        "            if self.render:\n",
        "                self.env.render()\n",
        "            transition = Transition(states=state.view(1, -1),\n",
        "                                                  actions=action.view(1, -1),\n",
        "                                                  rewards=torch.FloatTensor([[reward]]),\n",
        "                                                  next_states=next_state.view(1, -1),\n",
        "                                                  done=torch.FloatTensor([[done]]),\n",
        "                                                  exploration_statistics=exploration_statistics)\n",
        "            self.buffer.add(transition)\n",
        "            trajectory.append(transition)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "                break\n",
        "            else:\n",
        "                state = next_state\n",
        "        return trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def discrete_trust_region_update(actor_gradients, action_probabilities, average_action_probabilities):\n",
        "        \"\"\"\n",
        "        Update the actor gradients so that they satisfy a linearized KL constraint with respect\n",
        "        to the average actor-critic network. See Section 3.3 of the paper for details.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_gradients : tuple of torch.Tensor's\n",
        "            The original gradients.\n",
        "        action_probabilities\n",
        "            The action probabilities according to the current actor-critic network.\n",
        "        average_action_probabilities\n",
        "            The action probabilities according to the average actor-critic network.\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of torch.Tensor's\n",
        "            The updated gradients.\n",
        "        \"\"\"\n",
        "        negative_kullback_leibler = - ((average_action_probabilities.log() - action_probabilities.log())\n",
        "                                       * average_action_probabilities).sum(-1)\n",
        "        kullback_leibler_gradients = torch.autograd.grad(negative_kullback_leibler.mean(),\n",
        "                                                         action_probabilities, retain_graph=True)\n",
        "        updated_actor_gradients = []\n",
        "        for actor_gradient, kullback_leibler_gradient in zip(actor_gradients, kullback_leibler_gradients):\n",
        "            scale = actor_gradient.mul(kullback_leibler_gradient).sum(-1).unsqueeze(-1) - TRUST_REGION_CONSTRAINT\n",
        "            scale = torch.div(scale, actor_gradient.mul(actor_gradient).sum(-1).unsqueeze(-1)).clamp(min=0.)\n",
        "            updated_actor_gradients.append(actor_gradient - scale * kullback_leibler_gradient)\n",
        "        return updated_actor_gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMiAqHrI_7l"
      },
      "source": [
        "### Run agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kltwuUSnJEt2"
      },
      "outputs": [],
      "source": [
        "def run_agent(shared_brain, render=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Run the agent.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shared_brain : brain.Brain\n",
        "        The shared brain the agents will use and update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    local_agent = DiscreteAgent(shared_brain, render, verbose)\n",
        "    local_agent.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(ENVIRONMENT_NAME=\"CartPole-v1\",model_path=\"model.pkl\",num_episodes=1000):\n",
        "  \n",
        "  # Create the environment\n",
        "  env = gym.make(ENVIRONMENT_NAME)\n",
        "  \n",
        "  ## Environment parameters\n",
        "  action_space = env.action_space\n",
        "  state_space = env.observation_space\n",
        "  ACTION_SPACE_DIM = action_space.n\n",
        "  STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "\n",
        "  ## load model here\n",
        "  model = torch.load(model_path)\n",
        "  model.eval()\n",
        "\n",
        "  # Set the number of episodes to run\n",
        "  num_episodes = 1000\n",
        "\n",
        "  # Create lists to store the returns for each episode\n",
        "  returns = []\n",
        "\n",
        "  # Run the episodes\n",
        "  for episode in range(num_episodes):\n",
        "    # Reset the environment at the start of each episode\n",
        "    state = env.reset()\n",
        "    \n",
        "    # Initialize the episode return\n",
        "    episode_return = 0\n",
        "    episode_length = 0  \n",
        "    # Run the episode\n",
        "    done = False\n",
        "    while not done:\n",
        "      # Get the action from the model\n",
        "      action_prob,_  = model(torch.Tensor(state))\n",
        "      \n",
        "      # Step the environment with the action\n",
        "      next_state, reward, done, _ = env.step(torch.argmax(action_prob).numpy())\n",
        "      \n",
        "      # Update the episode return\n",
        "      episode_return += reward\n",
        "      episode_length += 1\n",
        "      # Update the state\n",
        "      state = next_state\n",
        "    # Add the episode return to the list of returns\n",
        "    returns.append([episode_return,episode_length]) \n",
        "#     print(episode,episode_return,episode_length)\n",
        "  return returns"
      ],
      "metadata": {
        "id": "vDvqXBjhlG8k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwJe8DyVblIM"
      },
      "source": [
        "### Init Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LlLEkzf2bq-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0007300a-47a9-4670-825f-58fc68f56ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ENVIRONMENT_NAME = 'CartPole-v1'\n",
        "# ENVIRONMENT_NAME = 'MountainCarContinuous-v0'\n",
        "\n",
        "env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "action_space = env.action_space\n",
        "state_space = env.observation_space\n",
        "env.close()\n",
        "del env\n",
        "\n",
        "ACTION_SPACE_DIM = action_space.n\n",
        "CONTROL = 'discrete'\n",
        "STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "# Parameters that work well for CartPole-v0\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "### this is hyperparameter\n",
        "REPLAY_BUFFER_SIZE = 100\n",
        "REPLAY_RATIO = 4\n",
        "MAX_EPISODES = 500\n",
        "OFF_POLICY_MINIBATCH_SIZE = 16\n",
        "MAX_REPLAY_SIZE = 200\n",
        "\n",
        "\n",
        "TRUNCATION_PARAMETER = 10\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "MAX_STEPS_BEFORE_UPDATE = 20\n",
        "NUMBER_OF_AGENTS = 2\n",
        "TRUST_REGION_CONSTRAINT = 1.\n",
        "TRUST_REGION_DECAY = 0.99\n",
        "ENTROPY_REGULARIZATION = 1e-3\n",
        "ACTOR_LOSS_WEIGHT = 0.1\n",
        "\n",
        "# Not used for discrete agents\n",
        "ORNSTEIN_UHLENBECK_NOISE_SCALE = None\n",
        "INITIAL_ORNSTEIN_UHLENBECK_NOISE_RATIO = None\n",
        "NUMBER_OF_EXPLORATION_EPISODES = None\n",
        "INITIAL_STANDARD_DEVIATION = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP0YfFDCbtTe"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir IS"
      ],
      "metadata": {
        "id": "mQ18bOEvlRoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5549c39-a1a0-497c-8888-5bf14d9f6824"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘IS’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu4ilTKobWZ9",
        "outputId": "9462a689-6273-4e0a-dc0d-1059fadc227e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "IS/CartPole-v1_100_1_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_1_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_2_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_4_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_100_8_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_1_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_2_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_4_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_250_8_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_1_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_2_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_5_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_4_9_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_0_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_1_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_2_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_3_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_4_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_6_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_7_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_8_IS_\n",
            "############################################################\n",
            "IS/CartPole-v1_500_8_9_IS_\n"
          ]
        }
      ],
      "source": [
        "for MAX_EPISODES in [100,250,500]:\n",
        "    for REPLAY_RATIO in [1,2,4,8]:\n",
        "            for itern in range(10):\n",
        "#                 try:\n",
        "                brain = DiscreteBrain()\n",
        "                local_agent = DiscreteAgent(brain, False,True)\n",
        "                local_agent.run()\n",
        "\n",
        "                length = brain.train_logs.qsize()\n",
        "                out = [brain.train_logs.get_nowait() for _ in range(length)]\n",
        "                train_result = pd.DataFrame(out,columns=[\"Expected Reward\",\"Episode length\",\"Episode number\"])\n",
        "\n",
        "                pathName = \"IS/\" + ENVIRONMENT_NAME + \"_\" + str(MAX_EPISODES) + \"_\" + str(REPLAY_RATIO) \\\n",
        "                            + \"_\" + str(itern) + \"_\" + \"IS_\"\n",
        "                print(\"############################################################\")\n",
        "                print(pathName)\n",
        "                ## save logs\n",
        "                train_result.to_csv( pathName + \"train_result.csv\",index=None)\n",
        "\n",
        "                ## save model\n",
        "                torch.save(brain.actor_critic, pathName + \"model.pkl\")\n",
        "\n",
        "                returns = test(ENVIRONMENT_NAME, model_path=  pathName+\"model.pkl\", num_episodes=1000)\n",
        "\n",
        "                test_results = pd.DataFrame(returns,columns=[\"Expected Rewards\",\"Episode Length\"])\n",
        "                test_results.to_csv(pathName+ \"test_result.csv\",index=None)\n",
        "#                 except:\n",
        "#                     print(\"-------------------------------ERROR ------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1qweDGocg9A"
      },
      "outputs": [],
      "source": [
        "250,4,9"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U2MDOxx5ILJ",
        "outputId": "d4e5e739-1a2f-4ccd-99ec-cdfc3f3d87a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "OdDxXKis_k-b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aICgss5ui4ZA"
      },
      "outputs": [],
      "source": [
        "out =[]\n",
        "for file in glob.glob(\"./IS/*test*.csv\"):\n",
        "    data = pd.read_csv(file)\n",
        "    out.append(file.split(\"_\")[1:4] + [data[\"Expected Rewards\"].mean(),data['Expected Rewards'].std()])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resdf = pd.DataFrame(out,columns=[\"Max_episodes\",\"Replay\",\"itern\",\"mean\",\"std\"]).groupby(by=[\"Max_episodes\",\"Replay\"],as_index=False).mean()"
      ],
      "metadata": {
        "id": "PUJdX03ni4ZB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "n-cgAxdMAON4",
        "outputId": "c6a3ae0a-57c0-4068-8d70-e74bddcb0692"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Max_episodes Replay     mean        std\n",
              "0           100      1  24.1022   8.802536\n",
              "1           100      2  19.8883   7.295012\n",
              "2           100      4  23.2624  10.939580\n",
              "3           100      8  28.4503  13.384068\n",
              "4           250      1  24.8722   8.462178\n",
              "5           250      2  25.0103  11.463606\n",
              "6           250      4  22.4186  13.110303\n",
              "7           250      8  14.2407   4.543888\n",
              "8           500      1  26.6887  11.916351\n",
              "9           500      2  20.9840  13.809516\n",
              "10          500      4  24.7648  12.831675\n",
              "11          500      8  21.1445   7.935195"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee737276-7a5d-4311-8250-7a0c409209be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_episodes</th>\n",
              "      <th>Replay</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>24.1022</td>\n",
              "      <td>8.802536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>19.8883</td>\n",
              "      <td>7.295012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>23.2624</td>\n",
              "      <td>10.939580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>8</td>\n",
              "      <td>28.4503</td>\n",
              "      <td>13.384068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>250</td>\n",
              "      <td>1</td>\n",
              "      <td>24.8722</td>\n",
              "      <td>8.462178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>25.0103</td>\n",
              "      <td>11.463606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>250</td>\n",
              "      <td>4</td>\n",
              "      <td>22.4186</td>\n",
              "      <td>13.110303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>14.2407</td>\n",
              "      <td>4.543888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>26.6887</td>\n",
              "      <td>11.916351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>500</td>\n",
              "      <td>2</td>\n",
              "      <td>20.9840</td>\n",
              "      <td>13.809516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>500</td>\n",
              "      <td>4</td>\n",
              "      <td>24.7648</td>\n",
              "      <td>12.831675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>500</td>\n",
              "      <td>8</td>\n",
              "      <td>21.1445</td>\n",
              "      <td>7.935195</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee737276-7a5d-4311-8250-7a0c409209be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee737276-7a5d-4311-8250-7a0c409209be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee737276-7a5d-4311-8250-7a0c409209be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in [\"100\",\"250\",\"500\"]:\n",
        "    plt.errorbar(resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"Replay\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"mean\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"std\"],label=\"Max episodes = \"+ episode)\n",
        "plt.xlabel(\"REPLAY RATIO\")\n",
        "plt.ylabel(\"Average Reward with std deviation\")\n",
        "plt.title(\"Max episodes for training vs Replay Ratio\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "e053b5ad-5fd6-4bc9-f85e-5b9444fa5a1e",
        "id": "8NicAyYdi4ZB"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f10de5431f0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSUfGt6N5cPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}