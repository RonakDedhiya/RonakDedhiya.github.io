{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SA0CDifItbZ"
      },
      "source": [
        "### Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEOTTPlTKllj",
        "outputId": "90fa8121-28a3-4489-b1f6-6279a4c2007c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wsNO3FuyIuOZ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Discrete as DiscreteSpace\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import multiprocessing as mp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEwiIU77IbFw"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fRFg13-oIWIf"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'next_states',\n",
        "                                       'done', 'exploration_statistics'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay buffer for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.episodes = deque([[]], maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    def add(self, transition):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            The transition to add.\n",
        "        \"\"\"\n",
        "        if self.episodes[-1] and self.episodes[-1][-1].done[0, 0]:\n",
        "            self.episodes.append([])\n",
        "        self.episodes[-1].append(transition)\n",
        "\n",
        "    def sample(self, batch_size, window_length=float('inf')):\n",
        "        \"\"\"\n",
        "        Sample a batch of trajectories from the buffer. If they are of unequal length\n",
        "        (which is likely), the trajectories will be padded with zero-reward transitions.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            The batch size of the sample.\n",
        "        window_length : int, optional\n",
        "            The window length.\n",
        "        Returns\n",
        "        -------\n",
        "        list of Transition's\n",
        "            A batched sampled trajectory.\n",
        "        \"\"\"\n",
        "        batched_trajectory = []\n",
        "        trajectory_indices = random.choices(range(len(self.episodes)-1), k=min(batch_size, len(self.episodes)-1))\n",
        "        trajectories = []\n",
        "        for trajectory in [self.episodes[index] for index in trajectory_indices]:\n",
        "            start = random.choices(range(len(trajectory)), k=1)[0]\n",
        "            trajectories.append(trajectory[start:start + window_length])\n",
        "        smallest_trajectory_length = min([len(trajectory) for trajectory in trajectories]) if trajectories else 0\n",
        "        for index in range(len(trajectories)):\n",
        "            trajectories[index] = trajectories[index][-smallest_trajectory_length:]\n",
        "        for transitions in zip(*trajectories):\n",
        "            batched_transition = Transition(*[torch.cat(data, dim=0) for data in zip(*transitions)])\n",
        "            batched_trajectory.append(batched_transition)\n",
        "        return batched_trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def extend(transition):\n",
        "        \"\"\"\n",
        "        Generate a new zero-reward transition to extend a trajectory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : Transition\n",
        "            A terminal transition which will become the new transition's previous\n",
        "            transition in the trajectory.\n",
        "        Returns\n",
        "        -------\n",
        "        Transition\n",
        "            The new transition that can be used to extend a trajectory.\n",
        "        \"\"\"\n",
        "        if not transition.done[0, 0]:\n",
        "            raise ValueError(\"Can only extend a terminal transition.\")\n",
        "        exploration_statistics = torch.ones(transition.exploration_statistics.size()) \\\n",
        "                                 / transition.exploration_statistics.size(-1)\n",
        "        transition = Transition(states=transition.next_states,\n",
        "                                actions=transition.actions,\n",
        "                                rewards=torch.FloatTensor([[0.]]),\n",
        "                                next_states=transition.next_states,\n",
        "                                done=transition.done,\n",
        "                                exploration_statistics=exploration_statistics)\n",
        "        return transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXtKkquIg_v"
      },
      "source": [
        "### Ornstein Uhlenbeck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "srY32EPTImkw"
      },
      "outputs": [],
      "source": [
        "class OrnsteinUhlenbeckProcess:\n",
        "    def __init__(self, theta, mu, sigma, time_scale=1e-1,\n",
        "                 size=1, initial_value=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.time_scale = time_scale\n",
        "        self.size = size\n",
        "        self.initial_value = initial_value if initial_value is not None else np.zeros(size)\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sample(self):\n",
        "        value = self.previous_value\n",
        "        value += self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        value += self.sigma * np.sqrt(self.time_scale) * np.random.normal(size=self.size)\n",
        "        return value\n",
        "\n",
        "    def reset(self):\n",
        "        self.previous_value = self.initial_value\n",
        "\n",
        "    def sampling_parameters(self):\n",
        "        mean = self.previous_value + self.theta * (self.mu - self.previous_value) * self.time_scale\n",
        "        sd = self.sigma * np.sqrt(self.time_scale) * np.ones((self.size,))\n",
        "        return mean, sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsR1jS81aVu0"
      },
      "source": [
        "### Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2gGuX7mCIzm6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, *input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def copy_parameters_from(self, source, decay=0.):\n",
        "        \"\"\"\n",
        "        Copy the parameters from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the parameters.\n",
        "        decay : float, optional\n",
        "            How much decay should be applied? Default is 0., which means the parameters\n",
        "            are completely copied.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter.data.copy_(decay * parameter.data + (1 - decay) * source_parameter.data)\n",
        "\n",
        "    def copy_gradients_from(self, source):\n",
        "        \"\"\"\n",
        "        Copy the gradients from another network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        source : ActorCritic\n",
        "            The network from which to copy the gradients.\n",
        "        \"\"\"\n",
        "        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n",
        "            parameter._grad = source_parameter.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igZXsvZ6aNaY"
      },
      "source": [
        "##### Discrete Actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UCeJLhvxaMn8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DiscreteActorCritic(ActorCritic):\n",
        "    \"\"\"\n",
        "    Discrete actor-critic network used in A3C and ACER.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_layer = torch.nn.Linear(STATE_SPACE_DIM, 32)\n",
        "        self.hidden_layer = torch.nn.Linear(32, 32)\n",
        "        self.action_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "        self.action_value_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        Compute a forward pass in the network.\n",
        "        Parameters\n",
        "        ----------\n",
        "        states : torch.Tensor\n",
        "            The states for which the action probabilities and the action-values must be computed.\n",
        "        Returns\n",
        "        -------\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action probabilities of the policy according to the actor.\n",
        "        action_probabilities : torch.Tensor\n",
        "            The action-values of the policy according to the critic.\n",
        "        \"\"\"\n",
        "        hidden = F.relu(self.input_layer(states))\n",
        "        hidden = F.relu(self.hidden_layer(hidden))\n",
        "        action_probabilities = F.softmax(self.action_layer(hidden), dim=-1)\n",
        "        action_values = self.action_value_layer(hidden)\n",
        "        return action_probabilities, action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_N8FWAlacq_"
      },
      "source": [
        "### Brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WQWMtWLZTFNL"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Brain:\n",
        "    \"\"\"\n",
        "    A centralized brain for the agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.actor_critic = None\n",
        "        self.average_actor_critic = None\n",
        "        self.train_logs = mp.Queue()\n",
        "\n",
        "class DiscreteBrain(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.actor_critic = DiscreteActorCritic()\n",
        "        self.actor_critic.share_memory()\n",
        "        self.average_actor_critic = DiscreteActorCritic()\n",
        "        self.average_actor_critic.share_memory()\n",
        "        self.average_actor_critic.copy_parameters_from(self.actor_critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxotsgUI9Qk"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "iOX6bbPBI-Ps"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent that learns an optimal policy using ACER.\n",
        "    Parameters\n",
        "    ----------\n",
        "    brain : brain.Brain\n",
        "        The brain to update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    def __init__(self, brain, render=False, verbose=False):\n",
        "        self.env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "        self.env.reset()\n",
        "        self.render = render\n",
        "        self.verbose = verbose\n",
        "        self.buffer = ReplayBuffer()\n",
        "        self.brain = brain\n",
        "        self.optimizer = torch.optim.Adam(brain.actor_critic.parameters(),\n",
        "                                          lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pv9UAMzbLhZ"
      },
      "source": [
        "#### Discrete Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xMxHxq-NbEEf"
      },
      "outputs": [],
      "source": [
        "class DiscreteAgent(Agent):\n",
        "    def __init__(self, brain, render=True, verbose=True):\n",
        "        super().__init__(brain, render, verbose)\n",
        "        \n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the agent for several episodes.\n",
        "        \"\"\"\n",
        "        for episode in range(MAX_EPISODES):\n",
        "            episode_rewards = 0\n",
        "            episode_length = 0\n",
        "            end_of_episode = False\n",
        "#             if self.verbose:\n",
        "#                 print(\"Episode #{}\".format(episode), end=\"\")\n",
        "            while not end_of_episode:\n",
        "                trajectory = self.explore(self.brain.actor_critic) \n",
        "                self.learning_iteration(trajectory)\n",
        "                end_of_episode = trajectory[-1].done[0, 0]\n",
        "                episode_rewards += sum([transition.rewards[0, 0] for transition in trajectory])\n",
        "                episode_length += len(trajectory)\n",
        "                for trajectory_count in range(np.random.poisson(REPLAY_RATIO)):\n",
        "                    trajectory = self.buffer.sample(OFF_POLICY_MINIBATCH_SIZE, MAX_REPLAY_SIZE)\n",
        "                    if trajectory:\n",
        "                        self.learning_iteration(trajectory)\n",
        "                        episode_length += len(trajectory)\n",
        "\n",
        "#             if self.verbose:\n",
        "#                 print(\", episode rewards {} - episode length {}\".format(episode_rewards,episode_length))\n",
        "            self.brain.train_logs.put([episode_rewards.numpy(),episode_length,episode])\n",
        "\n",
        "             \n",
        "    def learning_iteration(self, trajectory):\n",
        "        \"\"\"\n",
        "        Conduct a single discrete learning iteration. Analogue of Algorithm 2 in the paper.\n",
        "        \"\"\"\n",
        "        actor_critic = DiscreteActorCritic()\n",
        "        actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "\n",
        "        _, _, _, next_states, _, _ = trajectory[-1]\n",
        "        action_probabilities, action_values = actor_critic(Variable(next_states))\n",
        "        retrace_action_value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1)\n",
        "\n",
        "        for states, actions, rewards, _, done, exploration_probabilities in reversed(trajectory):\n",
        "            action_probabilities, action_values = actor_critic(Variable(states))\n",
        "            average_action_probabilities, _ = self.brain.average_actor_critic(Variable(states))\n",
        "            value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1) * (1. - done)\n",
        "            action_indices = Variable(actions.long())\n",
        "\n",
        "            importance_weights = action_probabilities.data / exploration_probabilities\n",
        "            \n",
        "            naive_advantage = action_values.gather(-1, action_indices).data - value\n",
        "            retrace_action_value = rewards + DISCOUNT_FACTOR * retrace_action_value * (1. - done)\n",
        "            retrace_advantage = retrace_action_value - value\n",
        "\n",
        "            # Actor\n",
        "            actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "                importance_weights.gather(-1, action_indices.data).clamp(max=TRUNCATION_PARAMETER) * retrace_advantage) \\\n",
        "                * action_probabilities.gather(-1, action_indices).log()\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n",
        "            #                           naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            # actor_loss = - ACTOR_LOSS_WEIGHT * Variable(importance_weights.clamp(min=0.) *\n",
        "            #                           naive_advantage*action_probabilities.data) * action_probabilities.log()\n",
        "            bias_correction = - ACTOR_LOSS_WEIGHT * Variable((1 - TRUNCATION_PARAMETER / importance_weights).clamp(min=0.) *\n",
        "                                      naive_advantage * action_probabilities.data) * action_probabilities.log()\n",
        "            actor_loss += bias_correction.sum(-1).unsqueeze(-1)\n",
        "            actor_gradients = torch.autograd.grad(actor_loss.mean(), action_probabilities, retain_graph=True)\n",
        "            actor_gradients = self.discrete_trust_region_update(actor_gradients, action_probabilities,\n",
        "                                                       Variable(average_action_probabilities.data))\n",
        "            action_probabilities.backward(actor_gradients, retain_graph=True)\n",
        "\n",
        "            # Critic\n",
        "            critic_loss = (action_values.gather(-1, action_indices) - Variable(retrace_action_value)).pow(2)\n",
        "            # critic_loss = (action_values.gather(-1, action_indices) - Variable(value)).pow(2)\n",
        "            critic_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            # Entropy\n",
        "            entropy_loss = ENTROPY_REGULARIZATION * (action_probabilities * action_probabilities.log()).sum(-1)\n",
        "            entropy_loss.mean().backward(retain_graph=True)\n",
        "\n",
        "            retrace_action_value = importance_weights.gather(-1, action_indices.data).clamp(max=1.) * \\\n",
        "                                   (retrace_action_value - action_values.gather(-1, action_indices).data) + value\n",
        "        self.brain.actor_critic.copy_gradients_from(actor_critic)\n",
        "        self.optimizer.step()\n",
        "        self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic, decay=TRUST_REGION_DECAY)\n",
        "        # self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic)\n",
        "        \n",
        "\n",
        "    def explore(self, actor_critic):\n",
        "        \"\"\"\n",
        "        Explore an environment by taking a sequence of actions and saving the results in the memory.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_critic : ActorCritic\n",
        "            The actor-critic model to use to explore.\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(self.env.env.state)\n",
        "        trajectory = []\n",
        "        for step in range(MAX_STEPS_BEFORE_UPDATE):\n",
        "            action_probabilities, *_ = actor_critic(Variable(state))\n",
        "            action = action_probabilities.multinomial(1)\n",
        "            action = action.data\n",
        "            exploration_statistics = action_probabilities.data.view(1, -1)\n",
        "            next_state, reward, done, _= self.env.step(action.numpy()[0])\n",
        "            next_state = torch.from_numpy(next_state).float()\n",
        "            if self.render:\n",
        "                self.env.render()\n",
        "            transition = Transition(states=state.view(1, -1),\n",
        "                                                  actions=action.view(1, -1),\n",
        "                                                  rewards=torch.FloatTensor([[reward]]),\n",
        "                                                  next_states=next_state.view(1, -1),\n",
        "                                                  done=torch.FloatTensor([[done]]),\n",
        "                                                  exploration_statistics=exploration_statistics)\n",
        "            self.buffer.add(transition)\n",
        "            trajectory.append(transition)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "                break\n",
        "            else:\n",
        "                state = next_state\n",
        "        return trajectory\n",
        "\n",
        "    @staticmethod\n",
        "    def discrete_trust_region_update(actor_gradients, action_probabilities, average_action_probabilities):\n",
        "        \"\"\"\n",
        "        Update the actor gradients so that they satisfy a linearized KL constraint with respect\n",
        "        to the average actor-critic network. See Section 3.3 of the paper for details.\n",
        "        Parameters\n",
        "        ----------\n",
        "        actor_gradients : tuple of torch.Tensor's\n",
        "            The original gradients.\n",
        "        action_probabilities\n",
        "            The action probabilities according to the current actor-critic network.\n",
        "        average_action_probabilities\n",
        "            The action probabilities according to the average actor-critic network.\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of torch.Tensor's\n",
        "            The updated gradients.\n",
        "        \"\"\"\n",
        "        negative_kullback_leibler = - ((average_action_probabilities.log() - action_probabilities.log())\n",
        "                                       * average_action_probabilities).sum(-1)\n",
        "        kullback_leibler_gradients = torch.autograd.grad(negative_kullback_leibler.mean(),\n",
        "                                                         action_probabilities, retain_graph=True)\n",
        "        updated_actor_gradients = []\n",
        "        for actor_gradient, kullback_leibler_gradient in zip(actor_gradients, kullback_leibler_gradients):\n",
        "            scale = actor_gradient.mul(kullback_leibler_gradient).sum(-1).unsqueeze(-1) - TRUST_REGION_CONSTRAINT\n",
        "            scale = torch.div(scale, actor_gradient.mul(actor_gradient).sum(-1).unsqueeze(-1)).clamp(min=0.)\n",
        "            updated_actor_gradients.append(actor_gradient - scale * kullback_leibler_gradient)\n",
        "        return updated_actor_gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMiAqHrI_7l"
      },
      "source": [
        "### Run agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kltwuUSnJEt2"
      },
      "outputs": [],
      "source": [
        "def run_agent(shared_brain, render=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Run the agent.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shared_brain : brain.Brain\n",
        "        The shared brain the agents will use and update.\n",
        "    render : boolean, optional\n",
        "        Should the agent render its actions in the on-policy phase?\n",
        "    verbose : boolean, optional\n",
        "        Should the agent print progress to the console?\n",
        "    \"\"\"\n",
        "    local_agent = DiscreteAgent(shared_brain, render, verbose)\n",
        "    local_agent.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(ENVIRONMENT_NAME=\"CartPole-v1\",model_path=\"model.pkl\",num_episodes=1000):\n",
        "  \n",
        "  # Create the environment\n",
        "  env = gym.make(ENVIRONMENT_NAME)\n",
        "  \n",
        "  ## Environment parameters\n",
        "  action_space = env.action_space\n",
        "  state_space = env.observation_space\n",
        "  ACTION_SPACE_DIM = action_space.n\n",
        "  STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "\n",
        "  ## load model here\n",
        "  model = torch.load(model_path)\n",
        "  model.eval()\n",
        "\n",
        "  # Set the number of episodes to run\n",
        "  num_episodes = 1000\n",
        "\n",
        "  # Create lists to store the returns for each episode\n",
        "  returns = []\n",
        "\n",
        "  # Run the episodes\n",
        "  for episode in range(num_episodes):\n",
        "    # Reset the environment at the start of each episode\n",
        "    state = env.reset()\n",
        "    \n",
        "    # Initialize the episode return\n",
        "    episode_return = 0\n",
        "    episode_length = 0  \n",
        "    # Run the episode\n",
        "    done = False\n",
        "    while not done:\n",
        "      # Get the action from the model\n",
        "      action_prob,_  = model(torch.Tensor(state))\n",
        "      \n",
        "      # Step the environment with the action\n",
        "      next_state, reward, done, _ = env.step(torch.argmax(action_prob).numpy())\n",
        "      \n",
        "      # Update the episode return\n",
        "      episode_return += reward\n",
        "      episode_length += 1\n",
        "      # Update the state\n",
        "      state = next_state\n",
        "    # Add the episode return to the list of returns\n",
        "    returns.append([episode_return,episode_length]) \n",
        "#     print(episode,episode_return,episode_length)\n",
        "  return returns"
      ],
      "metadata": {
        "id": "vDvqXBjhlG8k"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwJe8DyVblIM"
      },
      "source": [
        "### Init Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlLEkzf2bq-9",
        "outputId": "4cab00c8-f01f-445b-db95-48bea50a9b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ENVIRONMENT_NAME = 'CartPole-v1'\n",
        "# ENVIRONMENT_NAME = 'MountainCarContinuous-v0'\n",
        "\n",
        "env = gym.make(ENVIRONMENT_NAME, render_mode=\"rgb_array\")\n",
        "action_space = env.action_space\n",
        "state_space = env.observation_space\n",
        "env.close()\n",
        "del env\n",
        "\n",
        "ACTION_SPACE_DIM = action_space.n\n",
        "CONTROL = 'discrete'\n",
        "STATE_SPACE_DIM = state_space.shape[0]\n",
        "\n",
        "# Parameters that work well for CartPole-v0\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "### this is hyperparameter\n",
        "REPLAY_BUFFER_SIZE = 100\n",
        "REPLAY_RATIO = 4\n",
        "MAX_EPISODES = 500\n",
        "OFF_POLICY_MINIBATCH_SIZE = 16\n",
        "MAX_REPLAY_SIZE = 200\n",
        "\n",
        "\n",
        "TRUNCATION_PARAMETER = 10\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "MAX_STEPS_BEFORE_UPDATE = 20\n",
        "NUMBER_OF_AGENTS = 2\n",
        "TRUST_REGION_CONSTRAINT = 1.\n",
        "TRUST_REGION_DECAY = 0.99\n",
        "ENTROPY_REGULARIZATION = 1e-3\n",
        "ACTOR_LOSS_WEIGHT = 0.1\n",
        "\n",
        "# Not used for discrete agents\n",
        "ORNSTEIN_UHLENBECK_NOISE_SCALE = None\n",
        "INITIAL_ORNSTEIN_UHLENBECK_NOISE_RATIO = None\n",
        "NUMBER_OF_EXPLORATION_EPISODES = None\n",
        "INITIAL_STANDARD_DEVIATION = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP0YfFDCbtTe"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir acer"
      ],
      "metadata": {
        "id": "mQ18bOEvlRoI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cu4ilTKobWZ9",
        "outputId": "48f6d92f-1ccd-457d-a70f-c695c4dbc0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "acer/CartPole-v1_100_1_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_1_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_2_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_4_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_100_8_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_1_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_2_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_3_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_4_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_5_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_6_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_7_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_8_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_4_9_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_8_0_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_8_1_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_8_2_IS_\n",
            "############################################################\n",
            "acer/CartPole-v1_250_8_3_IS_\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-88745227d09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mlocal_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mlocal_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-fe5090254b7a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_of_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mend_of_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-fe5090254b7a>\u001b[0m in \u001b[0;36mlearning_iteration\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrace_action_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# critic_loss = (action_values.gather(-1, action_indices) - Variable(value)).pow(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for MAX_EPISODES in [100,250,500]:\n",
        "    for REPLAY_RATIO in [1,2,4,8]:\n",
        "            for itern in range(10):\n",
        "#                 try:\n",
        "                brain = DiscreteBrain()\n",
        "                local_agent = DiscreteAgent(brain, False,True)\n",
        "                local_agent.run()\n",
        "\n",
        "                length = brain.train_logs.qsize()\n",
        "                out = [brain.train_logs.get_nowait() for _ in range(length)]\n",
        "                train_result = pd.DataFrame(out,columns=[\"Expected Reward\",\"Episode length\",\"Episode number\"])\n",
        "\n",
        "                pathName = \"acer/\" + ENVIRONMENT_NAME + \"_\" + str(MAX_EPISODES) + \"_\" + str(REPLAY_RATIO) \\\n",
        "                            + \"_\" + str(itern) + \"_\" + \"IS_\"\n",
        "                print(\"############################################################\")\n",
        "                print(pathName)\n",
        "                ## save logs\n",
        "                train_result.to_csv( pathName + \"train_result.csv\",index=None)\n",
        "\n",
        "                ## save model\n",
        "                torch.save(brain.actor_critic, pathName + \"model.pkl\")\n",
        "\n",
        "                returns = test(ENVIRONMENT_NAME, model_path=  pathName+\"model.pkl\", num_episodes=1000)\n",
        "\n",
        "                test_results = pd.DataFrame(returns,columns=[\"Expected Rewards\",\"Episode Length\"])\n",
        "                test_results.to_csv(pathName+ \"test_result.csv\",index=None)\n",
        "#                 except:\n",
        "#                     print(\"-------------------------------ERROR ------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "250,2,4"
      ],
      "metadata": {
        "id": "314IUODMR93F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "OdDxXKis_k-b"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "M1qweDGocg9A"
      },
      "outputs": [],
      "source": [
        "out =[]\n",
        "for file in glob.glob(\"./acer/*test*.csv\"):\n",
        "    data = pd.read_csv(file)\n",
        "    out.append(file.split(\"_\")[1:4] + [data[\"Expected Rewards\"].mean(),data['Expected Rewards'].std()])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resdf = pd.DataFrame(out,columns=[\"Max_episodes\",\"Replay\",\"itern\",\"mean\",\"std\"]).groupby(by=[\"Max_episodes\",\"Replay\"],as_index=False).mean()"
      ],
      "metadata": {
        "id": "1U2MDOxx5ILJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "n-cgAxdMAON4",
        "outputId": "5a7791f4-5976-437f-820b-ca2aee559fc3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Max_episodes Replay      mean        std\n",
              "0          100      1   38.6322   9.396061\n",
              "1          100      2   23.7711   3.537152\n",
              "2          100      4   36.4138   7.569259\n",
              "3          100      8  107.7550  17.345119\n",
              "4          250      1   98.5919  10.551202\n",
              "5          250      2  138.8500   7.664121\n",
              "6          250      4  208.0048  16.709292\n",
              "7          250      8  179.6330  16.663645"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ea0f643-8ae8-49c6-8cf0-dcc7689ec7a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_episodes</th>\n",
              "      <th>Replay</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>38.6322</td>\n",
              "      <td>9.396061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>23.7711</td>\n",
              "      <td>3.537152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>36.4138</td>\n",
              "      <td>7.569259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>8</td>\n",
              "      <td>107.7550</td>\n",
              "      <td>17.345119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>250</td>\n",
              "      <td>1</td>\n",
              "      <td>98.5919</td>\n",
              "      <td>10.551202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>138.8500</td>\n",
              "      <td>7.664121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>250</td>\n",
              "      <td>4</td>\n",
              "      <td>208.0048</td>\n",
              "      <td>16.709292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>179.6330</td>\n",
              "      <td>16.663645</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ea0f643-8ae8-49c6-8cf0-dcc7689ec7a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ea0f643-8ae8-49c6-8cf0-dcc7689ec7a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ea0f643-8ae8-49c6-8cf0-dcc7689ec7a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in [\"100\",\"250\"]:\n",
        "    plt.errorbar(resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"Replay\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"mean\"],\n",
        "                resdf.loc[resdf[\"Max_episodes\"].isin([episode]),\"std\"],label=\"Max episodes = \"+ episode)\n",
        "plt.xlabel(\"REPLAY RATIO\")\n",
        "plt.ylabel(\"Average Reward with std deviation\")\n",
        "plt.title(\"Max episodes for training vs Replay Ratio\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "LSUfGt6N5cPY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "74bdebf7-2d1b-4292-d14c-6ec165d40fac"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe00d270d60>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnGgjqKqAJ_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3SA0CDifItbZ",
        "bEwiIU77IbFw",
        "uzXtKkquIg_v",
        "vsR1jS81aVu0",
        "E_N8FWAlacq_",
        "akxotsgUI9Qk",
        "-Pv9UAMzbLhZ",
        "dgMiAqHrI_7l"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}